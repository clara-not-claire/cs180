<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
    <style>
        body {
            padding: 100px;
            width: 1000px;
            margin: auto;
            text-align: left;
            font-weight: 300;
            font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
            color: rgb(55, 53, 47);
        }

        #desc p {
          font-size: 12px;
          text-align: center;
          padding-right: 20px;
        }

        h1, h2, h3, h4 {
            font-family: 'Source Sans Pro', sans-serif;
        }

        table {
            margin-left: 3vw;
        }

        div.padded {
            padding-top: 0px;
            padding-right: 100px;
            padding-bottom: 0.25in;
            padding-left: 100px;
        }

        .bold-italic {
            font-weight: bold;
            font-style: italic;
        }

        .image-row {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin-bottom: 20px;
        }

        .image-container {
            text-align: center;
            flex-basis: 20%;
            max-width: 100%;
            box-sizing: border-box;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .caption {
            margin-top: 5px;
            font-size: 12px;
            color: #555;
            text-align: center;
        }

        .sans {
            font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
        }

        .code {
            font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace;
        }

        .serif {
            font-family: Lyon-Text, Georgia, ui-serif, serif;
        }

        .mono {
            font-family: iawriter-mono, Nitti, Menlo, Courier, monospace;
        }

        .bold {
            font-weight: bold;
        }

        .grid-container {
            display: grid;
            grid-template-columns: repeat(4, 150px);
            grid-gap: 10px;
            justify-content: center;
        }

        .grid-item img {
            width: 100%;
            height: auto;
        }

        .grid-container-smol {
            display: grid;
            grid-template-columns: repeat(8, 100px);
            grid-gap: 10px;
            justify-content: center;
        }

        .flip-vertical {
            transform: scaleY(-1);
        }
    </style>
    <title>Clara | CS 180 Project 5</title>
    <meta http-equiv="content-type" name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
<body>
    <div align="center" style="margin:0; padding:0; display: flex;"></div>

    <article class="sans">
        <br />
        <h1 align="middle">CS180: Introduction to Computer Vision & Computation Photography</h1>
        <h1 align="middle">Project 5</h1>
        <h2 align="middle" class="bold-italic">Fun With Diffusion Models!</h3>
        <h3 align="middle">Clara Hung</h2>

        <h2 align="middle">Project Overview</h2>
        <p>In this project, we work with diffusion models!!</p>

        <!-- Example section with image row -->
        <h2 align="middle">Part 0: Setup / Sampling from the Model</h2>
        <p>Throughout this project, I use the random seed 11. Here are the three images generated from the starter code. Each of these is generated with 20 inference steps. Throughout this part of the project, we use DeepFloyd IF, a diffusion model trained by Stability AI.</p>

        <p>First, I generated images from the model with the default 20 steps.</p>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/source/oil.png" alt="Example image" />
                <p class="caption">An oil painting of a snowy mountain village</p>
            </div>
            
            <div class="image-container">
                <img src="./assets/source/hat.png" alt="Example image" />
                <p class="caption">A man wearing a hat</p>
            </div>

            <div class="image-container">
                <img src="./assets/source/rocket.png" alt="Example image" />
                <p class="caption">A rocket ship</p>
            </div>
        </div>

        <p>Here, I test increasing the number of inference steps to 100. The images contain more fine details than just using 20 steps. For example, the man photo has more distinct skin texture than the 20 step version. On the other hand, I think for simpler images, like the rocket, the 20 step version is better because it doesn't add extraneous details that make it look less realistic. The oil painting image had a similar quality. Oil paintings are known for their smooth, almost watercolor-like texture, and the 20 step version captures that well whereas the 100 step version is too precise.</p>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/source/oil100.png" alt="Example image" />
                <p class="caption">An oil painting of a snowy mountain village (100 steps)</p>
            </div>
            
            <div class="image-container">
                <img src="./assets/source/hat100.png" alt="Example image" />
                <p class="caption">A man wearing a hat (100 steps)</p>
            </div>

            <div class="image-container">
                <img src="./assets/source/rocket100.png" alt="Example image" />
                <p class="caption">A rocket ship (100 steps)</p>
            </div>
        </div>

        <p>Next, I try only using 5 inference steps. Here, the images are much more abstract and blurry, almost reflecting a pointilist style of art. While the general structure of each image is still there, there are less realistic details than the 20 step version. For example, the rocket ship and the hat are not very recognizable.</p>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/source/oil5.png" alt="Example image" />
                <p class="caption">An oil painting of a snowy mountain village (5 steps)</p>
            </div>
            
            <div class="image-container">
                <img src="./assets/source/hat5.png" alt="Example image" />
                <p class="caption">A man wearing a hat (5 steps)</p>
            </div>

            <div class="image-container">
                <img src="./assets/source/rocket5.png" alt="Example image" />
                <p class="caption">A rocket ship (5 steps)</p>
            </div>
        </div>
    <hr>
 
    <h2 align="middle">Part 1: Sampling Loops</h2>
    <p>This section of the project involves writing our own sampling loops that use pretrained DeepFloyd denoisers. For many of these sections, we will be using the Campinile test image:</p>

    <div class="image-container">
        <img src="./assets/source/CS180 Campanile.jpg" alt="Example image" height="256" width="256"/>
        <p class="caption">UC Berkeley's Campanile</p>
    </div>
    
    <h3 align="middle">1.1 Implementing the Forward Process</h3>
    <p>For the forward proess of a diffusion model, we take a clean image and add noies to it. The process is defined by computing: 
        <p>$$x_t = \sqrt{\bar\alpha_t} x_0 + \sqrt{1 - \bar\alpha_t} \epsilon \quad \text{where}~ \epsilon \sim N(0, 1)$$</p>

    where \(x_0\) is the clean image and \(x_t\) is the noisy image at timestep \(t\) that we get from sampling from a Gaussian with mean \(\sqrt{\bar\alpha_t} x_0\) and variance \( (1- \bar\alpha_t)\). \(\bar\alpha_t\) is given and should be close to 1 for small t and close to 0 for large t. 
    </p>
    <p>Test Image at the noise levels: <code>250, 500, 750</code></p>

    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/campanile_250.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Forward Process with Noise Level 250</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/campanile_500.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Forward Process with Noise Level 500</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/campanile_750.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Forward Process with Noise Level 750</p>
        </div>
    </div>

    <h3 align="middle">1.2 Classical Denoising</h3>
    <p>For each of the three noise levels in the previous part, I use a classical denoising model, e.g. gaussian blurring, to denoise the image. The results are as follows:</p>

    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/campanile_250_classical.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Classical Denoising at Noise Level 250</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/campanile_500_classical.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Classical Denoising at Noise Level 500</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/campanile_750_classical.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Classical Denoising at Noise Level 750</p>
        </div>
    </div>

    <h3 align="middle">1.3 One-Step Denoising</h3>
    <p>For each of the three noise levels in the previous part, I use the DeepFloyd stage 1 model to denoise the image by estimating the noise and removing it. We can obtain the clean image by solving for \(x_0\) in terms of \(x_t\) using the equation above. Since this model was trained with text conditioning, we also use a text prompt embedding `"a high quality photo"`. The results are as follows:</p>

    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/campanile_250_oneshot.png" alt="Example image" height="128" width="128"/>
            <p class="caption">One-Step Denoising at Noise Level 250</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/campanile_500_oneshot.png" alt="Example image" height="128" width="128"/>
            <p class="caption">One-Step Denoising at Noise Level 500</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/campanile_750_oneshot.png" alt="Example image" height="128" width="128"/>
            <p class="caption">One-Step Denoising at Noise Level 750</p>
        </div>
    </div>
    
    <h3 align="middle">1.4 Iterative Denoising</h3>

    <p>The results from one-step denoising were better than classical denoising, but they could be better since you can see the image is still blurry. Thus, we use the following equation with strided timsteps to obtain our clean noise estimate: </p>

    \[ x_{t'} = \frac{\sqrt{\bar\alpha_{t'}}\beta_t}{1 - \bar\alpha_t} x_0 + \frac{\sqrt{\alpha_t}(1 - \bar\alpha_{t'})}{1 - \bar\alpha_t} x_t + v_\sigma \]
        
    <p>Here are samples from different timesteps in the iterative denoising process:</p>
    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/campanile_t690.png" alt="Timestep 690" height="128" width="128"/>
            <p class="caption">Iterative Denoising at Timestep 690</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/campanile_t540.png" alt="Timestep 540" height="128" width="128"/>
            <p class="caption">Iterative Denoising at Timestep 540</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/campanile_t390.png" alt="Timestep 290" height="128" width="128"/>
            <p class="caption">Iterative Denoising at Timestep 390</p>
        </div>
    </div>

    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/campanile_t240.png" alt="Timestep 290" height="128" width="128"/>
            <p class="caption">Iterative Denoising at Timestep 240</p>
        </div>
        <div class="image-container">
            <img src="./assets/results/campanile_t90.png" alt="Timestep 90" height="128" width="128"/>
            <p class="caption">Iterative Denoising at Timestep 90</p>
        </div>
    </div>

    <p>Here is a comparison of the final results using different denoising methods.</p>

    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/campanile_iterative.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Final Result: Iterative Denoising</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/campanile_oneshot.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Final Result: One-Step Denoising</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/campanile_classical.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Final Result: Classical Denoising</p>
        </div>
    </div>

    <h3 align="middle">1.5 Diffusion Model Sampling</h3>
    <p>So far, in the previous parts, we've only used the diffusion model to denoise an image. We can also generate images from scratch by sampling from the model. We are effectively denoising pure noise. </p>
    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/sampled1.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Generated Sample 1</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/sampled2.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Generated Sample 2</p>
        </div>
    </div>

    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/sampled3.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Generated Sample 3</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/sampled4.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Generated Sample 4</p>
        </div>
    </div>

    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/sampled5.png" alt="Example image" height="128" width="128"/>
            <p class="caption">Generated Sample 5</p>
        </div>
    </div>

    <h3 align="middle">1.6 Classifier-Free Guidance (CFG)</h3>
    <p>The generated images in the previous section are ok but not super high quality, and some of them don't make sense. Thus, to improve image quality, we use a technique called Classifier-Free Guidance (CFG) where we compute both a conditional and unconditional noise estimate \(\epsilon_c\) and \(\epsilon_u\). Our new estimate is now: \[\epsilon = \epsilon_u + \gamma
        (\epsilon_c - \epsilon_u)\]
    where \(\gamma\) controls the strength of the CFG. When \(\gamma = 0\), we get an unconditional noise estimate. When \(\gamma = 1\), we get a conditional noise estimate. Generally, we want to look at \(\gamma > 1\). Below are image samples from CFG. Noise how they are more high quality and realistic!</p>

    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/generated1.png" alt="Example image" height="128" width="128"/>
            <p class="caption">CFG Generated Image 1</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/generated2.png" alt="Example image" height="128" width="128"/>
            <p class="caption">CFG Generated Image 2</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/generated3.png" alt="Example image" height="128" width="128"/>
            <p class="caption">CFG Generated Image 3</p>
        </div>
    </div>

    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/generated4.png" alt="Example image" height="128" width="128"/>
            <p class="caption">CFG Generated Image 4</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/generated5.png" alt="Example image" height="128" width="128"/>
            <p class="caption">CFG Generated Image 5</p>
        </div>
    </div>

    <h3 align="middle">1.7 Image-to-image Translation</h3>
    <p>Now, what we are going to do is to take the original test image, add a little bit of noise, then force it back to the original image wihtout any conditioning. At noise levels, we should get an image similar to the test image. Effectively, the model "edits" the image. Notice how we get increasingly closer to the test image! Recall our indices go from high to low noise levels. </p>
        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/noise1.png" alt="Example image" height="128" width="128"/>
                <p class="caption">Image Translation with Index 1</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/noise3.png" alt="Example image" height="128" width="128"/>
                <p class="caption">Image Translation with Index 3</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/noise5.png" alt="Example image" height="128" width="128"/>
                <p class="caption">Image Translation with Index 5</p>
            </div>
        </div>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/noise7.png" alt="Example image" height="128" width="128"/>
                <p class="caption">Image Translation with Index 7</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/noise10.png" alt="Example image" height="128" width="128"/>
                <p class="caption">Image Translation with Index 10</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/noise20.png" alt="Example image" height="128" width="128"/>
                <p class="caption">Image Translation with Index 20</p>
            </div>
        </div>

        <h4 align="middle">1.7.1 Editing Hand-Drawn and Web Images</h4>
        <p>Another thing we can do is start with a nonrealistic image or some web image and project it onto the natural image manifold. The model will try to force the image into a more "realistic" looking image. Below, I tried with a web images and hand-drawn scribbles!</p>
        <p><b>Web Image: </b>I tried using a random image of bear clipart on the internet.</p>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/source/bear.jpg" alt="Web image" height="128" width="128"/>
                <p class="caption">Original Clipart</p>
            </div>
        </div>
        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/hand1_noise1.png" alt="Web image with noise 1" height="128" width="128"/>
                <p class="caption">Web Image Translation with Noise Level 1</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/hand1_noise3.png" alt="Web image with noise 3" height="128" width="128"/>
                <p class="caption">Web Image Translation with Noise Level 3</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/hand1_noise5.png" alt="Web image with noise 5" height="128" width="128"/>
                <p class="caption">Web Image Translation with Noise Level 5</p>
            </div>
        </div>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/hand1_noise7.png" alt="Web image with noise 7" height="128" width="128"/>
                <p class="caption">Web Image Translation with Noise Level 7</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/hand1_noise10.png" alt="Web image with noise 10" height="128" width="128"/>
                <p class="caption">Web Image Translation with Noise Level 10</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/hand1_noise20.png" alt="Web image with noise 20" height="128" width="128"/>
                <p class="caption">Web Image Translation with Noise Level 20</p>
            </div>
        </div>

        <p><b>Hand-Drawn Image #1:</b> Here, I drew an image of a simple looking sprout. Notice how the model thinks it looks like a bird, which I can see.</p>
        <div class="image-row"></div>
            <div class="image-container">
                <img src="./assets/source/original3.png" alt="Web image" height="128" width="128"/>
                <p class="caption">Original Clipart</p>
            </div>
        </div>
        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/hand1_noise1.png" alt="Hand-drawn image 1 with noise 1" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 1 with Noise Level 1</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/web1_noise3.png" alt="Hand-drawn image 1 with noise 3" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 1 with Noise Level 3</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/web1_noise5.png" alt="Hand-drawn image 1 with noise 5" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 1 with Noise Level 5</p>
            </div>
        </div>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/web1_noise7.png" alt="Hand-drawn image 1 with noise 7" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 1 with Noise Level 7</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/web1_noise10.png" alt="Hand-drawn image 1 with noise 10" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 1 with Noise Level 10</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/web1_noise20.png" alt="Hand-drawn image 1 with noise 20" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 1 with Noise Level 20</p>
            </div>
        </div>

        <p><b>Hand-Drawn Image #2: </b>here, I tried drawing an image of a coffee mug. The model thinks it looks like a lady with dark hair.</p>
        <div class="image-row"></div>
            <div class="image-container">
                <img src="./assets/source/original2.png" alt="Web image" height="128" width="128"/>
                <p class="caption">Original Clipart</p>
            </div>
        </div>
        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/hand2_noise1.png" alt="Hand-drawn image 2 with noise 1" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 2 with Noise Level 1</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/hand2_noise3.png" alt="Hand-drawn image 2 with noise 3" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 2 with Noise Level 3</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/hand2_noise5.png" alt="Hand-drawn image 2 with noise 5" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 2 with Noise Level 5</p>
            </div>
        </div>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/hand2_noise7.png" alt="Hand-drawn image 2 with noise 7" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 2 with Noise Level 7</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/hand2_noise10.png" alt="Hand-drawn image 2 with noise 10" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 2 with Noise Level 10</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/hand2_noise20.png" alt="Hand-drawn image 2 with noise 20" height="128" width="128"/>
                <p class="caption">Hand-Drawn Image 2 with Noise Level 20</p>
            </div>
        </div>
        <h4 align="middle">1.7.2 Inpainting</h4>
        <p>The same procedure can be used to implement in-painting, where we choose a mask and ask the model to fill in the image, leaving everything inside the mask alone but replace everything outside the edit.</p>

        <p>Below is an example of the image, mask, and area to replace. This was the only sample where I actually saved the intermediate outputs (RIP), but please trust me on this! I created three sample inpaintings below.</p>
        <div class="image-row"></div>
            <div class="image-container">
                <img src="./assets/results/image.png" alt="Inpainting result 1" height="128" width="128"/>
                <p class="caption">Image</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/mask.png" alt="Inpainting result 2" height="128" width="128"/>
                <p class="caption">Mask</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/toreplace.png" alt="Inpainting result 3" height="128" width="128"/>
                <p class="caption">To Replace</p>
            </div>
        </div>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/inpaint_1.png" alt="Inpainting result 1" height="128" width="128"/>
                <p class="caption">Test Image Result</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/inpaint_2.png" alt="Inpainting result 2" height="128" width="128"/>
                <p class="caption">Bear Result</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/inpaint_3.png" alt="Inpainting result 3" height="128" width="128"/>
                <p class="caption">Tree Result</p>
            </div>
        </div>

        <h4 align="middle">1.7.3 Text-Conditional Image-to-image Translation</h4>
        <p>Next, we will do a similar thing but guide the projection onto the manifold of images using a text prompt to control that projection. The image should gradually look more like the original image but also like the text prompt.</p>

        <p><b>Example 1: Test Image with Rocket Embedding</b></p>
        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/text_noise1.png" alt="Text-conditional image with noise 1" height="128" width="128"/>
                <p class="caption">Noise Level 1</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/text_noise3.png" alt="Text-conditional image with noise 3" height="128" width="128"/>
                <p class="caption">Noise Level 3</p>
            </div>
            <div class="image-container">
                <img src="./assets/results/text_noise5.png" alt="Text-conditional image with noise 5" height="128" width="128"/>
                <p class="caption">Noise Level 5</p>
            </div>
        </div>
        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/text_noise7.png" alt="Text-conditional image with noise 7" height="128" width="128"/>
                <p class="caption">Noise Level 7</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/text_noise10.png" alt="Text-conditional image with noise 10" height="128" width="128"/>
                <p class="caption">Noise Level 10</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/text_noise20.png" alt="Text-conditional image with noise 20" height="128" width="128"/>
                <p class="caption">Noise Level 20</p>
            </div>
        </div>
        <p><b>Example 2: Green plains with waterfall embedding</b></p>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/text2_noise1.png" alt="Text-conditional image 2 with noise 1" height="128" width="128"/>
                <p class="caption">Noise Level 1</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/text2_noise3.png" alt="Text-conditional image 2 with noise 3" height="128" width="128"/>
                <p class="caption">Noise Level 3</p>
            </div>
            <div class="image-container">
                <img src="./assets/results/text2_noise5.png" alt="Text-conditional image 2 with noise 5" height="128" width="128"/>
                <p class="caption">Noise Level 5</p>
            </div>
        </div>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/text2_noise7.png" alt="Text-conditional image 2 with noise 7" height="128" width="128"/>
                <p class="caption">Noise Level 7</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/text2_noise10.png" alt="Text-conditional image 2 with noise 10" height="128" width="128"/>
                <p class="caption">Noise Level 10</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/text2_noise20.png" alt="Text-conditional image 2 with noise 20" height="128" width="128"/>
                <p class="caption">Noise Level 20</p>
            </div>
        </div>
        <p><b>Example 3: Bear with Dog Embedding</b></p>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/text3_noise1.png" alt="Text-conditional image 3 with noise 1" height="128" width="128"/>
                <p class="caption">Noise Level 1</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/text3_noise3.png" alt="Text-conditional image 3 with noise 3" height="128" width="128"/>
                <p class="caption">Noise Level 3</p>
            </div>
            <div class="image-container">
                <img src="./assets/results/text3_noise5.png" alt="Text-conditional image 3 with noise 5" height="128" width="128"/>
                <p class="caption">Noise Level 5</p>
            </div>
        </div>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/text3_noise7.png" alt="Text-conditional image 3 with noise 7" height="128" width="128"/>
                <p class="caption">Noise Level 7</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/text3_noise10.png" alt="Text-conditional image 3 with noise 10" height="128" width="128"/>
                <p class="caption">Noise Level 10</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/text3_noise20.png" alt="Text-conditional image 3 with noise 20" height="128" width="128"/>
                <p class="caption">Noise Level 20</p>
            </div>
        </div>


    <h3 align="middle">1.8 Visual Anagrams</h3>

    <p>Now, we can build off of everything we've learned so far to implement Visual Anagrams to create optical illusions with diffusion models. We can create images that look like something in one orientation but something else in another orientation by computing the noise of each embedding, with one right-side-up and one upside-down. Then we average the two noise estimates. Mathematically, it looks like:
        \[\epsilon_2 = \text{flip}(\text{UNet}(\text{flip}(x_t), t, p_2))\]
        \[\epsilon_2 = \text{flip}(\text{UNet}(\text{flip}(x_t), t, p_2))\]
        \[\epsilon = (\epsilon_1 + \epsilon_2) / 2\]
    </p>
    
    <p><b>Visual Anagram #1: </b></p>
    <ul>
        <li>Upright: "an oil painting of people around a campfire"</li>
        <li>Upside-down: "an oil painting of an old man"</li>
    </ul>
    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/campfire_man.png" alt="Example image" />
            <p class="caption">An Oil Painting of People Around a Campfire (Upright View)</p>
        </div>
        
        <div class="image-container">
            <img src="./assets/results/campfire_man.png" alt="Example image" class="flip-vertical"/>
            <p class="caption">An Oil Painting of an Old Man (Inverted View)</p>
        </div>
    </div>

    <p><b>Visual Anagram #2: </b></p>
    <ul>
        <li>Upright: "a coastal village with boats"</li>
        <li>Upside-down: "a mountain landscape with trees"</li>
    </ul>
    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/village_coast.png" alt="Example image" />
            <p class="caption">A Coastal Village with Boats (Upright View)</p>
        </div>
        
        <div class="image-container">
            <img src="./assets/results/village_coast.png" alt="Example image" class="flip-vertical"/>
            <p class="caption">A Mountain Landscape with Trees (Inverted View)</p>
        </div>
    </div>

    <p><b>Visual Anagram #3: </b></p>
    <ul>
        <li>Upright: "a lithograph of waterfalls"</li>
        <li>Upside-down: "an oil painting of a snowy mountain village"</li>
    </ul>
    <div class="image-row">
        <div class="image-container">
            <img src="./assets/results/mountain_waterfall.png" alt="Example image" />
            <p class="caption">Caption for upright image</p>
        </div>
        
        <div class="image-container">
            <img src="./assets/results/mountain_waterfall.png" alt="Example image" class="flip-vertical"/>
            <p class="caption">Caption for upside-down image</p>
        </div>
    </div>

    <h3 align="middle">1.9 Hybrid Images</h3>

    <p>We're almost at the end!! Finally! We can implement Factorized Diffusion to create hybrid frequency images just like we did in Project 2! To create a hybrid image, we use a similar process as 1.8 but rather than averaging, we pass the noise estimate of one prompt emebedding through a low-pass filter and another through a high-pass filter so we can see different images at close and far distances. Below are my examples! For the final one, you can see that there is a dog in the nature.</p>

        <div class="image-row">
            <div class="image-container">
                <img src="./assets/results/skull_waterfall.png" alt="Example image" height="128" width="128"/>
                <p class="caption">Hybrid image of a skull and waterfall. LF: Skull; HF: Waterfall</p>
            </div>
            
            <div class="image-container">
                <img src="./assets/results/rocket_pencil.png" alt="Example image" height="128" width="128"/>
                <p class="caption">Hybrid image of a pencil and rocket. LF: Rocket; HF: Pencil</p>
            </div>

            <div class="image-container">
                <img src="./assets/results/water_dog.png" alt="Example image" height="128" width="128"/>
                <p class="caption">Hybrid image of a dog and a waterfall. LF: Waterfall; HF: Dog</p>
            </div>
        </div>

    <hr>
    <h2 align="middle">Part 1: Training a Single-Step Denoiser</h2>
    <p> In this part, we train our own diffusion model on MNIST. Our goal is to train a denoiser \(D_{\theta}\) such that it maps a noisy image \(z\) to a clean image \(x\). We do this by optimizing over the L2 Loss:

        \[L = \mathbb{E}_{z,x} \|D_{\theta}(z) - x\|^2\]

    We train our denoiser by generating data pairs of \((z, x)\) For the training batch, we can generate our noisy image from our clean image using the noising process:
    \[z = x + \sigma \epsilon,\quad \text{where }\epsilon \sim N(0, I)\]
    </p>

    <p>Below are visualizations of the noising process over \(\sigma = [0.0, 0.2, 0.4,
        0.5, 0.6, 0.8, 1.0]\)</p>
        <div class="image-container">
            <img src="./assets/results/fig3.png" alt="Out of distribution test result" height="500" width="500"/>
            <p class="caption">Visualization of the noise process</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/fig6_training_loss.png" alt="Out of distribution test result" />
            <p class="caption">Log-scale loss curve of UNET training</p>
        </div>

        <p>Here are visualizations of my results after the 1st and 5th epochs. Notice how more epochs lead to better looking results. In these results, I randomly picked digits, but in retrospect, it probably would have been better to choose the same digits to visualise. To be fair, the speci did not specify that I should choose the same digits, so oh wells, I'm kind of tired of UNETs.</p>
        <div class="image-container">
            <img src="./assets/results/fig5_epoch_1.png" alt="Out of distribution test result" height="300" width="300"/>
            <p class="caption">Results after Epoch 1</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/fig4_epoch_5.png" alt="Out of distribution test result" height="300" width="300"/>
            <p class="caption">Results after Epoch 5</p>
        </div>

        <p>One other test we can do is to test on out of distribution images. Our denoiser was trained on MNIST digist noised with \(\sigma = 0.5\), but what happens to the denoiser on different sigmas that it wasn't trained for? Below is an example of the UNET performance on varying levels of sigma. Notice that performance is best for noise levels less than 0.5.</p>
        <div class="image-container">
            <img src="./assets/results/fig7.png" alt="Out of distribution test result" height="600"/>
            <p class="caption">Out of Distribution Test Result</p>
        </div>

    <hr>

    <h2 align="middle">Part 2: Training a Diffusion Model</h2>
    <p>Can you believe it? The end is near!! Now, we are ready to train a UNET model to iteratively denoise an image. We will use DDPM in this part. Rather than denoising the image directly, we will train a UNET to estimate the noise in the image. Then, we can use the DDPM equations like we did in Part A to remove the moise from the image. We noise the image according to:
        \[x_t = \sqrt{\bar\alpha_t} x_0 + \sqrt{1 - \bar\alpha_t} \epsilon
        \quad \text{where}~ \epsilon \sim N(0, 1).\]
    </p>

    <p>While we could train separate UNETs for each time-step we are after, it is much easier for us to train a single UNET conditioned on a given timestep, giving us a final objective function of:
        \[L = \mathbb{E}_{z,x} \|\epsilon_{\theta}(z, t) - \epsilon\|^2\]
    </p>

    <p>For this process, we define a list of \(\alpha, \bar\alpha, \beta\) values with \(T = 300\), batch size 128, hidden dimensions 64, the Adam optimizer with an initial LR of 1e-3, an exponential LR decay scheduler with gamma of \(0.1^{(1.0 / \text{num_epochs})}\). We train for 20 epoch since this is a more difficult task than the previous part.</p>

    <p>Here is a plot of my loss over the training process:</p>
    <div class="image-container">
        <img src="./assets/results/training_loss_2.png" alt="Out of distribution test result" />
        <p class="caption">Log-scale loss curve of Time-Conditioned Training/p>
    </div>

    <p>We can also sample from the UNET using a process similar to sampling in part A. For this sampling loop, we implement the DDPM equations. Below are some samples across different epochs. You can see that the digits don't look great, but their quality improves across epochs.</p>

        <div class="image-container">
            <img src="./assets/results/fig8_epoch_1.png" alt="Example image" />
            <p class="caption">Results after Epoch 1</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/fig8_epoch_5.png" alt="Example image" />
            <p class="caption">Results after Epoch 5</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/fig8_epoch_10.png" alt="Example image" />
            <p class="caption">Results after Epoch 10</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/fig8_epoch_15.png" alt="Example image" />
            <p class="caption">Results after Epoch 15</p>
        </div>

        <div class="image-container">
            <img src="./assets/results/fig8_epoch_20.png" alt="Example image" />
            <p class="caption">Results after Epoch 20</p>
        </div>

    <p>How can we fix these digits? Recall, perhaps from 189, that we can greatly improve our results if we add class-conditioning since each digit has its unique characteristics that can be learned a whole lot easier if we ask the UNET to condition on the class. Like we noticied in part A, for class-conditioning, we have to add CFG for the results to be good. For class-conditioned networks, we have to slightly tweak our network by adding a one-hot vector for our classes, which are the digits from 0-9. Additionally, we add a 10% dropout for the class vector by setting it to zero 10% of the time. Now, our UNET is conditioned on both time and class. For training, we use the same hyperparameters and LR scheduler as in the time-conditioned model. Below is my loss curve for training:</p>

    <div class="image-container">
        <img src="./assets/results/training_loss_3.png" alt="Out of distribution test result" />
        <p class="caption">Log-scale loss curve of Time-Conditioned Training with CFG</p>
    </div>

    <p>Below are some samples across different epochs, where I draw 4 instances of each digits. You can see that the digits look much better than before. The quality of the digits improves across epochs as the shape becomes more defined and the strokes become slightly thinner and less like a toddler drew it..</p>

    <div class="image-container">
        <img src="./assets/results/fig9_epoch_1.png" alt="Example image" />
        <p class="caption">Results after Epoch 1</p>
    </div>

    <div class="image-container">
        <img src="./assets/results/fig9_epoch_5.png" alt="Example image" />
        <p class="caption">Results after Epoch 5</p>
    </div>

    <div class="image-container">
        <img src="./assets/results/fig9_epoch_10.png" alt="Example image" />
        <p class="caption">Results after Epoch 10</p>
    </div>

    <div class="image-container">
        <img src="./assets/results/fig9_epoch_15.png" alt="Example image" />
        <p class="caption">Results after Epoch 15</p>
    </div>

    <div class="image-container">
        <img src="./assets/results/fig9_epoch_20.png" alt="Example image" />
        <p class="caption">Results after Epoch 20</p>
    </div>

    <hr>

    </article>
</body>
</html>