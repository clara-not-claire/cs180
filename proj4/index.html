<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
    <style>
        body {
            padding: 100px;
            width: 1000px;
            margin: auto;
            text-align: left;
            font-weight: 300;
            font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
            color: rgb(55, 53, 47);
        }

        #desc p {
          font-size: 12px;
          text-align: center;
          padding-right: 20px;
        }

        h1,
        h2,
        h3,
        h4 {
            font-family: 'Source Sans Pro', sans-serif;
        }

        table {
            margin-left: 3vw;
        }

        div.padded {
            padding-top: 0px;
            padding-right: 100px;
            padding-bottom: 0.25in;
            padding-left: 100px;
        }

        .bold-italic {
            font-weight: bold;
            font-style: italic;
        }

        .image-row {
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            gap: 10px;
        }

        .image-container {
            text-align: center;
            flex-basis: 30%;
            max-width: 100%;
            box-sizing: border-box;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .caption {
            margin-top: 5px;
            font-size: 12px;
            color: #555;
            text-align: center;
        }

        .sans {
            font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
        }

        .code {
            font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace;
        }

        .serif {
            font-family: Lyon-Text, Georgia, ui-serif, serif;
        }

        .mono {
            font-family: iawriter-mono, Nitti, Menlo, Courier, monospace;
        }

        .bold {
            font-weight: bold;
        }
        .grid-container {
            display: grid;
            grid-template-columns: repeat(4, 150px); /* 4 columns */
            grid-gap: 10px; /* Space between images */
            justify-content: center;

        }
        .grid-item img {
            width: 100%;
            height: auto; /* Maintain aspect ratio */
        }

        .grid-container-smol {
            display: grid;
            grid-template-columns: repeat(8, 100px); /* 4 columns */
            grid-gap: 10px; /* Space between images */
            justify-content: center;

        }
    </style>
    <title>Clara | CS 180 Project 4</title>
    <meta http-equiv="content-type" name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
<body>
    <div align="center" style="margin:0; padding:0; display: flex;"></div>

    <article class="sans">
        <br />
        <h1 align="middle">CS180: Introduction to Computer Vision & Computation Photography</h1>
        <h1 align="middle">Project 4</h1>
        <h2 align="middle" class="bold-italic">Stitching Photo Mosaics</h3>
        <h3 align="middle">Clara Hung</h2>

        <h2 align="middle">Project Overview</h2>
        <p>The goal of this project is to get hands-on experience with different methods of image warping applied to image mosaicing. Trhough this, we will create an image mosaic by first taking our own photographs, then register, projective warp, resample, and composite images. Then, we will automate correspondence-finding so we can generate panoramas automatically. As for Part A, it's homography time!</p>

        <b><h2 align="middle">Image Warping and Mosaicing</h2></b>

        <h2 align="middle">1. Shooting Photos</h2>
        <p>See photos below!</p>
        <hr />

        <h2 align="middle">2. Recover Homographies</h2>
        <p>As a refresher, here is how we can solve for the homography matrix given a set of points! A homography performs the transformation \(p' = Hp\), where \(p'\) is the original set of points and \text{p'} are the points you are trying to warp to. If we have a homography matrix:</p>
        <p>
            $$
            H = \begin{pmatrix}
                h_{1} & h_{2} & h_{3} \\
                h_{4} & h_{5} & h_{6} \\
                h_{7} & h_{8} & 1
                \end{pmatrix} 
            $$
            </p>
            <p>and a set of points:</p>
            <p>
                $$
                \begin{pmatrix}
                    x_{1} & y_{1} & 1 \\
                    x_{2} & y_{2} & 1 \\
                    x_{3} & y_{3} & 1 \\
                    x_{4} & y_{4} & 1
                \end{pmatrix}
                $$
            </p>

            <p>We can solve for the homography matrix by setting up the following equations and solving via least squares:</p>
            <p>
                $$
                \begin{bmatrix} 
                    x_1 & y_1 & 1 & 0 & 0 & 0 & -x'_1 x_1 & -x'_1 y_1 \\
                    0 & 0 & 0 & x_1 & y_1 & 1 & -y'_1 x_1 & -y'_1 y_1 \\
                    x_2 & y_2 & 1 & 0 & 0 & 0 & -x'_2 x_2 & -x'_2 y_2 \\
                    0 & 0 & 0 & x_2 & y_2 & 1 & -y'_2 x_2 & -y'_2 y_2 \\
                    & & & & \vdots & & & \\
                \end{bmatrix}
                \begin{bmatrix} 
                    h_1 \\ h_2 \\ h_3 \\ h_4 \\ h_5 \\ h_6 \\ h_7 \\ h_8 
                \end{bmatrix} = 
                \begin{bmatrix} 
                    x'_1 \\ y'_1 \\ x'_2 \\ y'_2 \\ \vdots 
                \end{bmatrix}
                $$
                </p>                
            <p>In the end, we get:</p>
            <p>
                \[
                \begin{bmatrix} x' \\ y' \\ w \end{bmatrix} = 
                \begin{bmatrix} h_1 & h_2 & h_3 \\ h_4 & h_5 & h_6 \\ h_7 & h_8 & 1 \end{bmatrix} 
                \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}
                \]
                </p>
                
                <p>which becomes:</p>
                
                <p>
                \[
                \begin{bmatrix} \frac{x'}{w} \\ \frac{y'}{w} \end{bmatrix}
                \]
                </p>                

        </p>

        <p>For this project, I used <a href="https://cal-cs180.github.io/fa23/hw/proj3/tool.html">this tool</a> to help me define each of the correspondance points in the images.</p>
        <hr />

        <h2 align="middle">3. Image Rectification</h2>
        <p>A simple way to test if our homography works is by rectifying an image. Say I had a photo of a sign viewed from an angle:</p>

        <div class="image-row">
            <div class="image-container">
                <img src="assets/source/coffee.jpeg" alt="Sign" />
                <p class="caption">Coffee Sign Viewed from an Angle</p>
            </div>
        </div>
        
        <p>But I want to view it head-on. I can compute the homography transform from the coordinate system of the original image to the coordinate system of a rectified image. Now, I will be able to see the sign as if I were looking at it directly!</p>

        <div class="image-row">
            <div class="image-container">
                <img src="assets/results/warped_coffee.jpg" alt="Rectified Sign" />
                <p class="caption">Rectified Coffee Sign</p>
            </div>
        </div>

        <p>Here's another example applied to the license plate "ICHBIN". The actual license plate is small, but you can see how the license plate is now facing us.</p>

        <div class="image-row">
            <div class="image-container">
                <img src="assets/source/ichbin.jpeg" alt="License Plate" />
                <p class="caption">License Plate "ICHBIN"</p>
            </div>

            <div class="image-container">
                <img src="assets/results/warped_ichbin.jpg" alt="License Plate" />
                <p class="caption">Rectified "ICHBIN"</p>
            </div>
        </div>

        <p>And another example applied to this book talk poster. Fun fact, I will be moderating this talk! You should come!</p>

        <div class="image-row">
            <div class="image-container">
                <img src="assets/source/booktalk.png" alt="License Plate" />
                <p class="caption">Book Talk</p>
            </div>

            <div class="image-container">
                <img src="assets/results/warped_booktalk.jpg" alt="License Plate" />
                <p class="caption">Rectified Booktalk</p>
            </div>
        </div>

        <p>A couple things I learned here:</p>
        <ul>
            <li>Make sure the corners of the surface you're trying to rectify are in the same order as your rectified corners.</li>
            <li>Remember to choose your points such that, when you use <code>polygon</code> to recover your bounding box, the polygon's lines don't cross. That is, choose the points in an order such that the polygon has as large angles as possible.</li>
            <li>Use the <code>cv2</code> homography functions to debug.</li>
        </ul>

        <hr />

        <h2 align="middle">4. Blend Images Into a Mosaic</h2>
        <p>Here comes the hard part! To create the mosaic of images, I based it off of the method we discussed in lecture. 
            First, I chose one image as the reference image and computed the homography to warp all other images to the reference image. 
            Then, I created a mosaic canvas that was large enough to fit all the images. The way I did this is by finding the corners of the warped images and the reference image, then finding the minimum and maximum x and y values. This allowed me to find the width and height of the mosaic canvas. Once I did that, I created masks that were the size of this new canvas and filled the mask according to the non-zero values of the warped image and the original image. For this step, you have to be very careful in remembering to shift either the warped image or the reference image to the new canvas. The reason for this is because sometimes, your homography transform may result in a negative value in your new coordinate systems. You want to shift the negative values to positive but also account for this in the rest of your calculations.
        </p>

        <p>To blend the images, I used the two-band blending method from lecture. To reduce the presence of seams and ghosting, I used <code>scipy.ndimage.distance_transform_edt</code> to weight the mask pixel values relative to how far they were from the borders. I originally tried using the <code>cv2.distanceTransform</code> function but debugging that became really annoying due to it's lack of useful error messages. The reason why the borders of the images will produce seams is because during the gaussian blurring process, the blur kernel will take in black pixels from the edges, leading to a shading effect.</p>
        
        <p>Here is the general flow for blending the two images together:</p>
        <ol>
            <li>Compute the homography of the warped image to the reference image.</li>
            <li>Compute the corners of the warped image and the reference image</li>
            <li>Compute the size of the mosaic canvas</li>
            <li>Compute the mask for the warped image and the reference image</li>
            <li>Compute the distance transform for the mask</li>
            <li>Find the low and high frequencies fo each image</li>
            <li>Compute a weighted average of the low and high frequencies together using their masks.</li>
            <li>Combine the low and high frequencies together</li>
            <li>Blend the images together</li>
        </ol>

        <p>Here is an example of what the distance transform mask looks like for my Chicago Skyline images in Part B.</p>

        <div class="image-row">
            <div class="image-container">
                <img src="assets/results/mask_warp.jpg" alt="Mask" />
                <p class="caption">Mask for Warped Image</p>
            </div>
            <div class="image-container">
                <img src="assets/results/mask_ref.jpg" alt="Mask" />
                <p class="caption">Mask for Reference Image</p>
            </div>
        </div>

        <p>Here are some example photos I took!</p>

        <p><strong>Sunset:</strong> The sunset image worked a lot better than I expected!</p>
        <div class="image-row">
            <div class="image-container">
                <img src="assets/source/sunset1.jpeg" alt="Sunset" />
                <p class="caption">Sunset 1</p>
            </div>

            <div class="image-container">
                <img src="assets/results/blended_sunset.jpg" alt="Sunset" />
                <p class="caption">Sunset Mosaic</p>
            </div>

            <div class="image-container">
                <img src="assets/source/sunset2.jpeg" alt="Sunset" />
                <p class="caption">Sunset 2</p>
            </div>
        </div>

        <p><strong>Grass:</strong> Here, you can see a mild crease in the sky. It's not super obvious unless you squint, but I think that is something I can work on improving for Part B. Perhaps my blending is absorbing some of the border into the blend.</p>
        <div class="image-row">
            <div class="image-container">
                <img src="assets/source/half-religion1.jpeg" alt="Sunset" />
                <p class="caption">Pacific School of Religion 1</p>
            </div>

            <div class="image-container">
                <img src="assets/results/blended_religion.jpg" alt="Sunset" />
                <p class="caption">Pacific School of Religion Mosaic</p>
            </div>

            <div class="image-container">
                <img src="assets/source/half-religion2.jpeg" alt="Sunset" />
                <p class="caption">Pacific School of Religion 2</p>
            </div>
        </div>

        <p><strong>Philly:</strong> As you can see here, there is a little bit of blurriness at the bottom of the image, and the cars are mismatched. I believe this is because I was standing next to a busy street, so the cars naturally moved. Thus, in my second image, the cars shifted position! As we are limited by a 2D image, we cannot capture the cars in time. </p>

        <div class="image-row">
            <div class="image-container">
                <img src="assets/source/half-philly1.JPG" alt="Sunset" />
                <p class="caption">Philly 1</p>
            </div>

            <div class="image-container">
                <img src="assets/results/blended_philly.jpg" alt="Sunset" />
                <p class="caption">Philly Mosaic</p>
            </div>

            <div class="image-container">
                <img src="assets/source/half-philly2.JPG" alt="Sunset" />
                <p class="caption">Phlly 2</p>
            </div>
        </div>
        <hr />

        <h2 align="middle">Feature-matching and Autostitching</h2>
        <p>For the second part of the project, we are using the methods outlined in Brown et al's paper <a href="https://inst.eecs.berkeley.edu/~cs180/fa24/hw/proj4/Papers/MOPS.pdf">“Multi-Image Matching using Multi-Scale Oriented Patches”</a>, specifically sections 2, 3, 4, and 5. The goal of this section is we want to be able to automatically stich images into a mosaic since, as we learned in part a, choosing correspondance points is super tedious.</p>
        <hr>

        <h2 align="middle">1. Detecting corners</h2>
        <p>For corner detection, I used the <b>Harris Interest Point Detector</b> in Section 2 of the paper. To identify the corners, I used the code in <code>harris.py</code>. I converted my image to grayscale and used <code>get_harris_corners</code> to retreive the harris score <code>h</code> and <code>corners</code>. The function returns the scores as <code>(y, x)</code> but I wanted to use the points in the form <code>(x, y)</code> so I transposed the returned array.</p>
        <hr>

        <h2 align="middle">2. Extracting feature descriptors</h2>
        <p>For feature extraction, I followed Section 3's <b>Adaptive Non-Maximal Suppression</b> (ANMS) with <code>c_robust = 0.9</code> (as recommended in the paper) and <code>num_corners = 100</code>.</p>

        <p>To compute the actual ANMS score, followed the following procedure: </p>
        <ol>
            <li>I obtained the scores of each corner by indexing <code>h[corners[:, 0], corners[:, 1]]</code>.</li>
            <li>Then, I computed the pairwise distances of the corners using the <code>dist2</code> function provided in the <code>harris.py</code> starter code.</li>
            <li>Next, I created a mask to determine if \( f(x_i) < c_{\text{robust}} \cdot f(x_j) \) by using array broadcasting <code>scores[:, np.newaxis] < (c_robust * scores[np.newaxis, :])</code>. This essentially compares the strength of each of the corners.</li>
            <li>Applied the mask to my distance matrix of corner strength.</li>
            <li>Set zero elements of the matrix to be infinity. This is useful since we perform a minimization operation in the next step. (If you forget to do this, you may end up with 0-1 cornerse and be very confused.)</li>
            <li>To compute the minimum suppression radius, which minimizes the squared distances, I take <code>np.min(masked_dists, axis=1)</code>, then obtain the sorted indices based on suppression radius from largest to smallest using <code>(-radii).argsort()</code>. Then, I use that to obtain the sorted corners and select the first <code>num_corners</code> points as my ANMS points.</li>
        </ol>

        <p>Below is an example of the result of the Harris points vs the ANMS points!</p>

        <div class="image-row">
            <div class="image-container">
                <img src="assets/results/city1-corners.jpg" alt="Harris Corners" />
                <p class="caption">Harris Corners</p>
            </div>

            <div class="image-container">
                <img src="assets/results/city1-anms.jpg" alt="ANMS Corners" />
                <p class="caption">ANMS Corners</p>
            </div>
        </div>
        <hr>

        <h2 align="middle">3. Extracting feature descriptors</h2>
        <p>To extract feature descriptors, I followed Section 4 of the paper. For every point in the ANMS points, I:</p>

        <ol>
            <li>Extracted a 40x40 patch around the point.</li>
            <li>Downsampled this patch to become 8x8.</li>
            <li>Normalized the intensity by subtracting the mean and dividing by the standard deviation of the patch.</li>
            <li>Flattened the patch to be <code>(1, 192)</code> using <code>features.reshape(features.shape[0], -1)</code></li>
        </ol>

        <p>Below are a sample of my features:</p>

        <div class="grid-container-smol" align="middle">
            <div class="grid-item">
                <img src="assets/results/1-match1.jpg" alt="1-match1"></div>
            <div class="grid-item">
                <img src="assets/results/2-match7.jpg" alt="2-match1"></div>
            <div class="grid-item">
                <img src="assets/results/2-match8.jpg" alt="1-match2"></div>
            <div class="grid-item">
                <img src="assets/results/2-match2.jpg" alt="2-match2"></div>
            <div class="grid-item"><img src="assets/results/2-match9.jpg" alt="1-match3"></div>
            <div class="grid-item"><img src="assets/results/2-match3.jpg" alt="2-match3"></div>
            <div class="grid-item"><img src="assets/results/2-match10.jpg" alt="1-match4"></div>
            <div class="grid-item"><img src="assets/results/2-match4.jpg" alt="2-match4"></div>
            <div class="grid-item"><img src="assets/results/2-match11.jpg" alt="1-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match5.jpg" alt="2-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match12.jpg" alt="1-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match6.jpg" alt="2-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match13.jpg" alt="1-match3"></div>
            <div class="grid-item"><img src="assets/results/2-match14.jpg" alt="2-match3"></div>
            <div class="grid-item"><img src="assets/results/2-match15.jpg" alt="1-match4"></div>
            <div class="grid-item"><img src="assets/results/2-match16.jpg" alt="2-match4"></div>
            <div class="grid-item"><img src="assets/results/2-match17.jpg" alt="1-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match18.jpg" alt="2-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match19.jpg" alt="1-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match20.jpg" alt="2-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match21.jpg" alt="2-match4"></div>
            <div class="grid-item"><img src="assets/results/2-match22.jpg" alt="1-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match36.jpg" alt="2-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match37.jpg" alt="1-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match38.jpg" alt="2-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match39.jpg" alt="2-match4"></div>
            <div class="grid-item"><img src="assets/results/2-match40.jpg" alt="1-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match24.jpg" alt="2-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match25.jpg" alt="1-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match23.jpg" alt="2-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match26.jpg" alt="2-match4"></div>
            <div class="grid-item"><img src="assets/results/2-match27.jpg" alt="1-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match28.jpg" alt="2-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match29.jpg" alt="1-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match30.jpg" alt="2-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match31.jpg" alt="1-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match32.jpg" alt="2-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match33.jpg" alt="1-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match34.jpg" alt="2-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match35.jpg" alt="2-match6"></div>
        </div>
        <hr>

        <h2 align="middle">4. Matching feature descriptors</h2>
        <p>To match feature descriptors, Section 5 of the paper, I did the following:</p>

        <ol>
            <li>Extracted the features and their corresponding points for both images.</li>
            <li>For each feature compute the pairwise differences between the features of the the two images. I again used numpy array broadcasting (<code>diff = features1[:, np.newaxis, :] - features2[np.newaxis, :, :]</code>), which produces a <code>(num_points, num_points, 192)</code> matrix.</li>
            <li>Take the squared differences and sum over all 192 pixels to get a 100x100 matrix of the sum of squared differences.</li>
            <li>Sor each row (feature) from smallest to largest using <code>np.sort(ssd)</code>.</li>
            <li>The paper uses something called a Lowe Score, which is basically a comparison between the first nearest neighbor (NN) distance with the second NN distance. To do this, I computed <code>lowe = nn_dists[:, 0] / nn_dists[:, 1]</code> across all features.</li>
            <li>Then, I created a mask <code>lowe <= lowe_threshold</code> so I could extract features lower than the Lowe score. This means that the feature is most closely matched with its 1NN. For my project, I used <code>lowe_threshold = 0.3</code>.</li>
            <li>I obtained the indices of the 1NN for each feature <code>ssd.argsort()[:,0]</code>, then create a list of the indices paired with their corresponding image 1 features via <code>np.stack([np.arange(0, closest.shape[0]), closest]).T</code>.</li>
            <li>Finally, I filtered the index, feature pairs using <code>matches[lowe_mask]</code>.</li>
        </ol>

        <p>I used a lowe threshold of 0.3. Below are the first 12 match examples! The pictures are blurry because I'm blowing up an 8x8 image to be viewable on the screen.</p>

        <div class="grid-container" align="middle">
            <div class="grid-item">
                <p>Source Image</p>
                <img src="assets/results/1-match1.jpg" alt="1-match1"></div>
            <div class="grid-item">
                <p>Destination Image</p>
                <img src="assets/results/2-match1.jpg" alt="2-match1"></div>
            <div class="grid-item">
                <p>Source Image</p>
                <img src="assets/results/1-match2.jpg" alt="1-match2"></div>
            <div class="grid-item">
                <p>Destination Image</p>
                <img src="assets/results/2-match2.jpg" alt="2-match2"></div>
            <div class="grid-item"><img src="assets/results/1-match3.jpg" alt="1-match3"></div>
            <div class="grid-item"><img src="assets/results/2-match3.jpg" alt="2-match3"></div>
            <div class="grid-item"><img src="assets/results/1-match4.jpg" alt="1-match4"></div>
            <div class="grid-item"><img src="assets/results/2-match4.jpg" alt="2-match4"></div>
            <div class="grid-item"><img src="assets/results/1-match5.jpg" alt="1-match5"></div>
            <div class="grid-item"><img src="assets/results/2-match5.jpg" alt="2-match5"></div>
            <div class="grid-item"><img src="assets/results/1-match6.jpg" alt="1-match6"></div>
            <div class="grid-item"><img src="assets/results/2-match6.jpg" alt="2-match6"></div>
        </div>
        <hr>

        <h2 align="middle">5. Use a robust method (RANSAC) for homography</h2>
        <p>To implement RANSAC for computing homographies, I implemented the procedure we talked about in lecture, using these steps:</p>

        <ol>
            <li>Using the matches obtained from Step 4 and the points from Step 2, I split my points into source and destination points <code>matches_src, matches_dst = points1[matches[:, 0]], points2[matches[:, 1]]</code>.</li>
            <li>Randomly select 4 points from the matches using <code>np.random.choice(src_pts.shape[0], size=4,replace=False)</code>.</li>
            <li>Compute the homography matrix using the 4 points using the <code>computeH</code> function from Part A.</li>
            <li>Convert the source points to homogenous form <code>(x, y, 1)</code>.</li>
            <li>Transform source points into destination coordinates.</li>
            <li>Unscale the transformed points, then remove the homogenous column.</li>
            <li>Compute the number of inliers by computing the distance between the points and the homography matrix <code>np.sqrt(np.sum((dst_pts - transformed_src) ** 2, axis=1))</code>. Create a mask by checking if the distance is less than a threshold. If it is, then the point is an inliers.</li>
            <li>Repeat steps 1-3 for <code>steps</code> iterations.</li>
            <li>Throughout all iterations, keep track of the mask with the maximum number of inliers. At the end, use <code>best_mask</code> to return the source and destination points that produce the best homography. These are our corresponding points like in Part A.</li>
        </ol>

        <p>I used a threshold of 0.8 and 1000 steps. Here are images of my final corresponding points (red points):</p>

        <div class="image-row">
            <div class="image-container">
                <img src="assets/results/src_anms.jpg" alt="1-matches" />
                <p class="caption">Source Image</p>
            </div>

            <div class="image-container">
                <img src="assets/results/dst_anms.jpg" alt="2-matches" />
                <p class="caption">Destination Image</p>
            </div>
        </div>
        <hr>

        <h2 align="middle">6. Final Mosaics</h2>
        <p>To create the final mosaics using auto-stitching, I used the best set of source and desination points from Step 5, computing the homography matrix using these points and blending them together like in Part A. Before I threw my points into homography, I had to remember to swap the x and y axes of my points due to the way they are stored. Here are some of my final mosaics!</p>

        <h3><b>The Bean at Night</b></h3>
        <p>For this image, there is a bit of a seam in the middle since the exposure of the left image is different from the right image. This is due to iPhone's auto-exposure control. The result would look better once I implement some exposure correction.</p>
        <div class="image-row">
            <div class="image-container">
                <img src="assets/source/bean1.jpeg" alt="The Bean" />
                <p class="caption">The Bean 1</p>
            </div>

            <div class="image-container">
                <img src="assets/results/blended_bean.jpg" alt="The Bean" />
                <p class="caption">The Bean Mosaic</p>
            </div>

            <div class="image-container">
                <img src="assets/source/bean2.jpeg" alt="The Bean" />
                <p class="caption">The Bean 2</p>
            </div>
        </div>

        <h3><b>The Chicago Theater</b></h3>
        <p>For this image, I had to crop out the cars at the bottom of the image. The cars moved between the two frames, which made my auto feature detection match to the cars, yet the cars were in different positions in both photos so the mosaic alignment would not work properly.</p>

        <div class="image-row">
            <div class="image-container">
                <img src="assets/source/chi1.jpeg" alt="The Chicago Theater" />
                <p class="caption">The Chicago Theater 1</p>
            </div>

            <div class="image-container">
                <img src="assets/results/blended_chi.jpg" alt="The Chicago Theater" />
                <p class="caption">The Chicago Theater Mosaic</p>
            </div>

            <div class="image-container">
                <img src="assets/source/chi2.jpeg" alt="The Chicago Theater" />
                <p class="caption">The Chicago Theater 2</p>
            </div>
        </div>

        <h3><b>Chicago Skyline</b></h3>
        <p>Here are photos I took during my last day in Chicago. It was on an early morning run near Lake Michigan!</p>

        <div class="image-row">
            <div class="image-container">
                <img src="assets/source/city1.jpeg" alt="The Chicago Theater" />
                <p class="caption">City 1</p>
            </div>

            <div class="image-container">
                <img src="assets/results/blended_city.jpg" alt="The Chicago Theater" />
                <p class="caption">Chicago Skyline Mosaic</p>
            </div>

            <div class="image-container">
                <img src="assets/source/city2.jpeg" alt="The Chicago Theater" />
                <p class="caption">City 2</p>
            </div>
        </div>

        <hr>
        <h2 align="middle">Reflection</h2>
        <p>I learned a lot in this project! It was really cool to see the history behind how panoramas are generated, and it gives me a greater appreciation for the work that has been put into the technology I use today. At first, I had trouble with my images due to mis-matched center of projections, but once I fixed those bugs, it was very exciting to see my mosaics come together. I previously worked with homographies in my research, but implementing it myself made me understand how they worked a lot more. I have new ideas on how to use homographies for rectifying a license plate collection that I have!</p>

        <p>In Part B, what I found interesting was how the quality of your image greatly affects your auto-generated points. For example, I kept getting poorly stitched images only to realize that it was because I was capturing a dynamic scene. My algorithm was picking up corners that were moving. Thus, I was aligning to moving targets rather than stationary targets.</p>
    </article>
</body>
</html>
